# HebbianCNNPyTorch

Super-simple Hebbian learning in multi-layer convolutional networks with PyTorch. Just implement a special loss whose gradient is equal to the Hebbian update! Ready-made expressions are available for plain Hebb's rule (dw ~= xy), Grossberg's Instar rule (dw ~= y(x-w)) and Oja's rule, (dw ~= y(x-wy))

Look at the [Jupyter notebook](https://github.com/ThomasMiconi/HebbianCNNPyTorch/blob/main/HebbGrad_Simple_Github.ipynb) for a simple implementation.

Preprint coming soon!
