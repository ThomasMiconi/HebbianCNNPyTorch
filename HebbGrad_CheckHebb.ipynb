{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "HebbGrad_CheckHebb.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO/z5+GmkHkd344pATLr0sB",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f56ab6b4c63d443d8d80b383707a5932": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_56cbbba3727348f09163e810e5eddaeb",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1349b5c88fb143498baf5e94ed2dff4c",
              "IPY_MODEL_5b83af51e4804a0eae9d45edebd5624a"
            ]
          }
        },
        "56cbbba3727348f09163e810e5eddaeb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1349b5c88fb143498baf5e94ed2dff4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1791e31153dc48d49bffbd15495588d8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_763ae84d2aca4cc8964da3faeee50bf6"
          }
        },
        "5b83af51e4804a0eae9d45edebd5624a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_98a052a0f0a544bcb7f1860c19519643",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [00:18&lt;00:00, 9249285.44it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5be94ddd8efd4493b8766ef56d84b184"
          }
        },
        "1791e31153dc48d49bffbd15495588d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "763ae84d2aca4cc8964da3faeee50bf6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "98a052a0f0a544bcb7f1860c19519643": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5be94ddd8efd4493b8766ef56d84b184": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThomasMiconi/HebbianCNNPyTorch/blob/main/HebbGrad_CheckHebb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6_IJ5_pB31a"
      },
      "source": [
        "This code verifies that the PyTorch-computed gradient updates are equal to hand-computed Hebbian updates, for the various rules being studied.\n",
        "\n",
        "We run the code for one single batch (and for a single layer), perform the backward pass, then (in the next cell) compare the computed gradient with the appropriate Hebbian update.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202,
          "referenced_widgets": [
            "f56ab6b4c63d443d8d80b383707a5932",
            "56cbbba3727348f09163e810e5eddaeb",
            "1349b5c88fb143498baf5e94ed2dff4c",
            "5b83af51e4804a0eae9d45edebd5624a",
            "1791e31153dc48d49bffbd15495588d8",
            "763ae84d2aca4cc8964da3faeee50bf6",
            "98a052a0f0a544bcb7f1860c19519643",
            "5be94ddd8efd4493b8766ef56d84b184"
          ]
        },
        "id": "T1tSLAo59ZTf",
        "outputId": "0a981de6-d048-4d5b-9787-a2befccb6613"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "import pdb\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import scipy\n",
        "from scipy import ndimage\n",
        "from scipy import linalg\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "from numpy import fft \n",
        "\n",
        "from scipy import io as spio\n",
        "\n",
        "\n",
        "\n",
        "NL=3; STRIDES=(12,1,1); POOLSTRIDES = (2, 2, 2); POOLDIAMS = (2, 2, 2) ; SIZES = (5, 3, 3) ; N = [100, 196, 400, 100]; K = [10, 1, 1]\n",
        "\n",
        "RULE = \"OJA\"\n",
        "\n",
        "# Should we use pruning / masking of weights? Probability for each layer.\n",
        "#PROBAMASK =  .75 # .97 \n",
        "#PROBAMASKS = (0, 0.5, 0.5, 0.5)\n",
        "#PROBAMASKS = (0, 0.9, 0.9, 0.9)\n",
        "#PROBAMASKS = (0, 0.95, 0.95, 0.95)\n",
        "#PROBAMASKS = (0, 0.98, 0.98, 0.98)\n",
        "#PROBAMASKS = (0, 0.999, 0.999, 0.999)\n",
        "PROBAMASKS = (0, 0., 0., 0.)\n",
        "# PROBAMASKS = (0, 0.99, 0.99)\n",
        "\n",
        "# Do we apply the ZCA to the whole image (IMAGE), individually to each patch through the unfolding/flattening convolution (\"method 2\" in the preprint) (FLAT), as a spatial filter (highly experimental, FILTER), or not at all? \n",
        "ZCA = 'NO' # FLAT, IMAGE, FILTER, None \n",
        "\n",
        "BINARIZEDPLAST = True   # Binarized WTA for plasticity?\n",
        "LEARNONLYL1 = False     # Learning only in Layer 1 (for tests)?\n",
        "USEFLAT = False         # Use the flattening/unfolding convolution (\"method 2\" in the preprint)?\n",
        "NORMW = True            # Constrain weights to norm 1?\n",
        "USETHRES = False         # Use adaptive thresholds?\n",
        "\n",
        "# Use Coates' \"triangle\" method? If TRAINTEST: Don't use it during Hebbian learning, but do use it during data collection for training/testing the linear classifier.\n",
        "USETRIANGLE = 'YES' # 'NO', 'YES', 'TRAINTEST'\n",
        "L1PEN =  [0, 0, 0] # ; L1PEN = [-0.0 * xx for xx in L1PEN] ; print(L1PEN[2])\n",
        "\n",
        "#TARGETRATE = [1.0/36, 2.0/400, 0.01, 0.01] \n",
        "#TARGETRATE = [0.05, 0.003, 0.01, 0.01] \n",
        "#TARGETRATE = [2/N[0], 2/N[1], 5/N[2]] # [3/xx for xx in N] \n",
        "TARGETRATE = [float(K[ii] / N[ii]) for ii in range(NL)]     # When using adaptive threshold, the target \"firing\" (k-WTA winning) rate must be K/N, otherwise trouble happens.\n",
        "\n",
        "BATCHSIZE=27 # 00\n",
        "\n",
        "RGB = True  # RGB or Grayscale?\n",
        "NBINPUTCHANNELS = 3 if RGB else 1\n",
        "\n",
        "\n",
        "CSIZE =    32       #  Size of input images\n",
        "FSIZE = 32          # Size of whitening filter. Should be CSIZE for ZCA=IMAGE, SIZES[0] for ZCA=FLAT.\n",
        "MIXDOG = 1.0        # For the Difference of Gaussians filter below. No longer used\n",
        "NBLEARNINGEPOCHS = 20   # Number of learning epochs. Add 2 epochs with frozen weights for collecting training/testing data for the linear classifier.\n",
        "LR = [ 0.001 / BATCHSIZE,  0.001 / BATCHSIZE,  10 * .001 / BATCHSIZE]   # Learning rates for each layer. We learn faster in the last layer, which has little data (only 2x2 outputs, only 1 winner for 400 filters each time)\n",
        "LR = [xx * 10.0 for xx in LR]\n",
        "MUTHRES =  3.0      #  Adaptation rate for adaptive thresholds\n",
        "\n",
        "\n",
        "# This computation of receptive field sizes may not be correct anymore.\n",
        "rfL1size = SIZES[0]  # Assumes no dilation in L1 \n",
        "rfL2size = STRIDES[0] * POOLSTRIDES[0]*(SIZES[1]-1)+SIZES[0]\n",
        "rfL3size = STRIDES[1] * POOLSTRIDES[1]*STRIDES[0] * POOLSTRIDES[0]* (SIZES[2]-1)+rfL2size\n",
        "print(\"RF sizes: L1:\", rfL1size, \"L2:\", rfL2size, \"L3:\", rfL3size )\n",
        "\n",
        "\n",
        "tic = time.time()\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "\n",
        "# PyTorch data loading boilerplate. Note the multiple conditions. \"UNLAB\" = unlabelled (Hebbian learning)\n",
        "\n",
        "if RGB:    \n",
        "    transform_zca = transforms.Compose([\n",
        "        torchvision.transforms.RandomHorizontalFlip(),\n",
        "    #torchvision.transforms.RandomRotation(45),\n",
        "            transforms.RandomCrop(FSIZE),  \n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    transform_unlab = transforms.Compose([\n",
        "        torchvision.transforms.RandomHorizontalFlip(),\n",
        "    #torchvision.transforms.RandomRotation(45),\n",
        "            transforms.RandomCrop(CSIZE),  # Larger image\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    transform_train = transforms.Compose([\n",
        "        torchvision.transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomCrop(CSIZE),  # Larger image\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "else:\n",
        "    transform_zca = transforms.Compose([\n",
        "    torchvision.transforms.Grayscale(),\n",
        "    torchvision.transforms.RandomHorizontalFlip(),\n",
        "    #torchvision.transforms.RandomRotation(45),\n",
        "    transforms.RandomCrop(FSIZE),  # Larger image\n",
        "    transforms.ToTensor(),\n",
        "    ])\n",
        "    transform_unlab = transforms.Compose([\n",
        "    torchvision.transforms.Grayscale(),\n",
        "    torchvision.transforms.RandomHorizontalFlip(),\n",
        "    #torchvision.transforms.RandomRotation(45),\n",
        "    transforms.RandomCrop(CSIZE),  # Larger image\n",
        "    transforms.ToTensor(),\n",
        "    ])\n",
        "    transform_train = transforms.Compose([\n",
        "    torchvision.transforms.Grayscale(),\n",
        "    torchvision.transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(CSIZE),  # Larger image\n",
        "    transforms.ToTensor(),\n",
        "    ])\n",
        "transform_test = transform_train\n",
        "# trainset = torchvision.datasets.CIFAR10(\n",
        "#    root='./data', train=True, download=True, transform=transform_train)\n",
        "# trainloader = torch.utils.data.DataLoader(\n",
        "#    trainset, batch_size=BATCHSIZE, shuffle=False)#, num_workers=2) # Also check out pin_memory if using GPU\n",
        "\n",
        "\n",
        "# STL10 data. Potentially useful for unsupervised learning !  Requires adjusting the size parameters.\n",
        "# unlabloader = trainloader = testloader = None\n",
        "# trainset = torchvision.datasets.STL10(\n",
        "#    root='./data', split='train', download=True, transform=transform_train)\n",
        "# testset = torchvision.datasets.STL10(\n",
        "#     root='./data', split='test', download=True, transform=transform_test)\n",
        "# unlabset = torchvision.datasets.STL10(\n",
        "#     root='./data', split='unlabeled', download=True, transform=transform_unlab)\n",
        "# zcaset = torchvision.datasets.STL10(\n",
        "#     root='./data', split='unlabeled', download=True, transform=transform_zca)\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "   root='./data', train=True, download=True, transform=transform_train)\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=transform_test)\n",
        "unlabset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transform_unlab)\n",
        "zcaset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transform_zca)\n",
        "\n",
        "\n",
        "zcaloader = torch.utils.data.DataLoader(\n",
        "    zcaset, batch_size=9000, shuffle=True, num_workers=2)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "   trainset, batch_size=BATCHSIZE, shuffle=True, num_workers=2) # Also check out pin_memory if using GPU\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=BATCHSIZE, shuffle=False, num_workers=2)\n",
        "unlabloader = torch.utils.data.DataLoader(\n",
        "    unlabset, batch_size=BATCHSIZE, shuffle=True, num_workers=2)\n",
        "\n",
        "\n",
        "# The Olshausen & Field images. Must be downloaded from Bruno Olshausen's website.\n",
        "#OP = spio.loadmat('IMAGES.mat')['IMAGES'] # Olshausen pictures, 512 * 512 * 10\n",
        "#OP = spio.loadmat('IMAGES_RAW.mat')['IMAGESr'].astype(np.float32) # Olshausen pictures, 512 * 512 * 10\n",
        "#OP = OP - np.min(OP); OP = OP / np.max(OP)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Initializations. Some of that stuff is not used anymore.\n",
        "\n",
        "# w has shape OutChannels, InChannels, H, W\n",
        "w=[]; masks = []\n",
        "wflat=[]\n",
        "wlat=[]; wlatdecay=[]\n",
        "meanfiringrates=[];meanfiringrates_p=[]; meanrealy=[]; meanabsprelimy=[]\n",
        "allthres=[]; allfirings = []; allfirings_p = []\n",
        "meancorry = [] ; meancorrp = []\n",
        "optimizers=[]\n",
        "DeccFilts = []\n",
        "thres=[]; C = []\n",
        "for numl in range(NL):\n",
        "\n",
        "    # For the flattening convolution, wflat are the fixed weights:\n",
        "    if numl == 0:\n",
        "        NIC = NBINPUTCHANNELS\n",
        "    else:\n",
        "        NIC = N[numl-1]\n",
        "    idx = 0\n",
        "    wf =  torch.zeros((NIC * SIZES[numl] * SIZES[numl], NIC, SIZES[numl], SIZES[numl]), requires_grad=False, device=device) \n",
        "    for nin in range(NIC):\n",
        "        for xin in range(SIZES[numl]):\n",
        "            for yin in range(SIZES[numl]):\n",
        "                wf.data[idx, nin, xin, yin] = 1\n",
        "                idx += 1\n",
        "    wflat.append(wf)\n",
        "\n",
        "    # The actual (learned) weights:\n",
        "    if  USEFLAT:\n",
        "        # when using the flattening / unfolding convolution :\n",
        "        wi = torch.randn((N[numl], NIC * SIZES[numl] * SIZES[numl], 1, 1), requires_grad=True, device=device) \n",
        "    else:\n",
        "        # When *not* using the flattening convolution\n",
        "        wi = torch.randn((N[numl], NIC, SIZES[numl], SIZES[numl]), requires_grad=True, device=device) \n",
        "\n",
        "    wi.data = wi.data * .01\n",
        "    \n",
        "    if True: # PROBAMASK > 0:\n",
        "        mi = (torch.rand_like(wi, requires_grad=False) > PROBAMASKS[numl]).float().to(device)\n",
        "        if numl == 0:\n",
        "            mi.fill_(1.0)\n",
        "        masks.append(mi)\n",
        "        wi.data = wi.data * mi.data\n",
        "    w.append(wi)\n",
        "    if NORMW:\n",
        "        # w has shape OutChannels, InChannels, H, W\n",
        "        w[numl].data =    w[numl].data  / (1e-10 + torch.sqrt(torch.sum(w[numl].data ** 2, dim=[1,2,3], keepdim=True)))\n",
        "    wlat.append( torch.zeros((N[numl], N[numl]), requires_grad=False, device=device) ) \n",
        "    wlatdecay.append( torch.zeros((N[numl], N[numl]), requires_grad=False, device=device) ) \n",
        "\n",
        "    C.append(0)\n",
        "    meancorry.append(torch.zeros((N[numl], N[numl]), requires_grad=False, device=device))\n",
        "    meancorrp.append(torch.zeros((N[numl], N[numl]), requires_grad=False, device=device))\n",
        "    allthres.append([])\n",
        "    allfirings.append([])\n",
        "    allfirings_p.append([])\n",
        "    meanfiringrates.append(torch.zeros((1,N[numl],1,1), requires_grad=False).to(device))\n",
        "    meanfiringrates_p.append(torch.zeros((1,N[numl],1,1), requires_grad=False).to(device))\n",
        "    meanrealy.append(torch.zeros((1,N[numl],1,1), requires_grad=False).to(device))\n",
        "    meanabsprelimy.append(torch.zeros((1,N[numl],1,1), requires_grad=False).to(device))\n",
        "    thres.append(torch.zeros_like(meanfiringrates[numl], requires_grad=False).to(device))\n",
        "    optimizers.append(optim.SGD((w[numl],), lr=LR[numl], momentum=0.0))\n",
        "    #optimizers.append(optim.Adam((w[numl],), lr=3e-4))\n",
        "\n",
        "realys = [0] * NL; prelimys = [0] * NL; xs = [0] * NL\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"Init time:\", time.time()-tic, \"Device:\", device)\n",
        "tic = time.time()\n",
        "firstpass=True\n",
        "nbbatches = -1\n",
        "wsav = []\n",
        "\n",
        "testaccs=[]; trainaccs=[]\n",
        "trainouts = []; trainouts_lin= []; trainouts_nomp = []; trainoutsq_l1 = []; trainoutsq_l2=[]; trainoutsq_l3=[]; traintargets = []; \n",
        "testouts = []; testouts_lin=[]; testouts_nomp=[]; testoutsq_l1 = []; testoutsq_l2=[]; testoutsq_l3=[]; testtargets = [] \n",
        "\n",
        "\n",
        "\n",
        "# Start the experiment !\n",
        "epoch = 0\n",
        "\n",
        "TESTING=False; TRAINING = False; UNLAB = True; \n",
        "zeloader = unlabloader\n",
        "\n",
        "numbatch=0\n",
        "\n",
        "x, targets = iter(zeloader).next()\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    \n",
        "    nbbatches += 1\n",
        "\n",
        "    # First, load and prepare the image data\n",
        "\n",
        "    x = x.to(device)\n",
        "    if TRAINING or TESTING:\n",
        "        targets = targets.to(device)\n",
        "\n",
        "    xorig = x.detach().clone()\n",
        "\n",
        "    x = x - torch.mean(x, dim=(1, 2,3),keepdim=True);  x = x / (1e-10 + torch.std(x, dim=(1, 2,3), keepdim=True)) \n",
        "\n",
        "\n",
        "    # Multiply whole images by full ZCA matrix, after whole-image normalization\n",
        "    if ZCA == 'IMAGE':\n",
        "        assert FSIZE == CSIZE\n",
        "        xshape = x.shape\n",
        "        t = x.reshape([xshape[0], -1])\n",
        "        t = t - torch.mean(t, axis=1, keepdims=True)\n",
        "        t = t / (1e-10 + torch.std(t, axis=1, keepdims=True))\n",
        "        \n",
        "        tzca = torch.matmul(zcamat, t.T).T\n",
        "        x = tzca.reshape(xshape)\n",
        "        # x=x.detach()\n",
        "    \n",
        "    xorig2 = x + 1e-10 # Just for debugging purposes\n",
        "\n",
        "\n",
        "numl = 0  # 1st layer only\n",
        "\n",
        "optimizers[numl].zero_grad()\n",
        "\n",
        "#Prepare the input to the layer\n",
        "with torch.no_grad():\n",
        "    # Normalize each element in the batch\n",
        "    x = x - torch.mean(x, dim=(1, 2,3),keepdim=True);  x = x / (1e-10 + torch.std(x, dim=(1, 2,3), keepdim=True)) \n",
        "\n",
        "    # If required, apply the \"flattening\"/unfolding convolution, separating each individual patch into a vector, and normalizing each of these.\n",
        "    if USEFLAT:\n",
        "        t = F.conv2d(x, wflat[numl]).detach()  # Should have size BS x nbinputchan*SIZES[0]*SIZES[0] x H_input x  W_input\n",
        "\n",
        "        t = t - torch.mean(t, axis=1, keepdims=True)\n",
        "        t = t / (1e-10 +  torch.std(t, axis=1, keepdims=True))\n",
        "        # Also if specified, also ZCA-whiten each individual input patch (note: this *does* improve performance a little bit, it seems; individual patch-normalization alone doesn't.)\n",
        "        if ZCA == 'FLAT' and numl == 0:\n",
        "            t = t.moveaxis(1,3)\n",
        "\n",
        "            t = torch.matmul(t[:,:, :, None, :], zcamat)\n",
        "            t = t.squeeze(3).moveaxis(3, 1)\n",
        "\n",
        "            t = t -  torch.mean(t, dim=1, keepdims=True)\n",
        "            t = t / (1e-10 + torch.std(t, dim=1, keepdims=True))\n",
        "\n",
        "        x = t\n",
        "\n",
        "    xs[numl] = x.clone()\n",
        "\n",
        "\n",
        "# Compute the FF input to the cells, which is also the first part of the computational graph (\"w*x\") and common to the \"real\" and \"surrogate\" outputs.\n",
        "prelimy = F.conv2d(x, w[numl], stride=STRIDES[numl]) \n",
        "\n",
        "\n",
        "# Now compute the \"real\" y output\n",
        "with torch.no_grad():\n",
        "    \n",
        "    prelimysav = prelimy.detach().clone()\n",
        "\n",
        "    if not USETHRES:\n",
        "        thres[numl].fill_(0)\n",
        "\n",
        "    realy = (prelimy - thres[numl])\n",
        "\n",
        "    # k-WTA\n",
        "    # y output has shape BatchSize x NbOutChannels x H x \n",
        "    tk = torch.topk(realy.data, K[numl], dim=1, largest=True)[0]\n",
        "    realy.data[realy.data < tk.data[:,-1,:,:][:, None, :, :]] = 0       \n",
        "\n",
        "    if BINARIZEDPLAST:\n",
        "        realy.data = (realy.data > 0).float()\n",
        "    \n",
        "    torch.clamp_(realy.data, max=50.0) # realy.data[realy.data>50.0] = 50.0 # Sould not happen.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Then we compute an auxiliary output yforgrad, which will be used solely to make gradient computations produce the desired Hebbian output:\n",
        "# w has shape OutChannels, InChannels, H, W\n",
        "# y output has shape BatchSize x NbOutChannels x H x \n",
        "\n",
        "# Note: you must not include thresholds here, because this is only to build the appropriate computational graph. \n",
        "# The actual values will come from realy, which does include thresholding.\n",
        "\n",
        "if RULE == \"INSTAR\":\n",
        "    yforgrad = prelimy - 1/2 * torch.sum(w[numl] * w[numl], dim=(1,2,3))[None,:, None, None]\n",
        "elif RULE == \"OJA\":\n",
        "    yforgrad = prelimy - 1/2 * torch.sum(w[numl] * w[numl], dim=(1,2,3))[None,:, None, None] * realy.data\n",
        "elif RULE == \"PLAINHEBB\":\n",
        "    yforgrad = prelimy\n",
        "else:\n",
        "    raise ValueError(\"Which Rule?\")\n",
        "\n",
        "\n",
        "\n",
        "yforgrad.data = realy.data # We force the value of yforgrad to be the \"correct\" y. Note: if using plain Hebb (yforgrad = prelimy), that means we're modifying the value of prelimy and thus can't reuse it in the future !\n",
        "\n",
        "\n",
        "# Compute the loss and perform the backward pass (if we are in the Hebbian / unlabeled phase)\n",
        "loss = ( torch.sum( -1/2 * yforgrad * yforgrad) )\n",
        "    # loss = ( torch.sum( yforgrad) )\n",
        "    # loss = ( torch.sum( -1/2 * w[0] * w[0]) )\n",
        "\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "print(\"Done!\")\n",
        "        \n",
        "        \n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RF sizes: L1: 5 L2: 53 L3: 149\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f56ab6b4c63d443d8d80b383707a5932",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=170498071.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Init time: 18.93275499343872 Device: cuda\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzfDBhwdWKki",
        "outputId": "f6775b83-162a-4337-a441-27fdf278f92a"
      },
      "source": [
        "w0 = w[0].data.cpu().numpy()\n",
        "g0 = w[0].grad.cpu().numpy()\n",
        "py = prelimysav.cpu().numpy()\n",
        "ry = realy.cpu().numpy()\n",
        "xx = x.cpu().numpy()\n",
        "xshape = xx.shape\n",
        "\n",
        "# First, other tests:\n",
        "\n",
        "print(\"w.shape:\", w0.shape, \"grad.shape:\", g0.shape, \"x.shape:\", xx.shape, \"py.shape:\", py.shape)\n",
        "print(g0[0,0,0,0])\n",
        "z = 0\n",
        "\n",
        "print(\"Actual py (channel 0, pos 0,0, batch position 0):\", py[0,0,0, 0])\n",
        "\n",
        "for col in range(xshape[2]):\n",
        "    for row in range(xshape[3]):\n",
        "        if col < 5  and row < 5:\n",
        "            for chan in range(1):\n",
        "                z += w0[0, chan, row,col] * xx[0, chan, row, col]\n",
        "print(\"Computed py:\", z)\n",
        "            \n",
        "posmaxy = np.argmax(py[0, :, 0, 0])\n",
        "\n",
        "\n",
        "print(\"Position max prelimy (batch pos 0, pos 0,0):\", posmaxy, \" - py at this pos:\", py[0, posmaxy, 0, 0])\n",
        "print(\"======\")\n",
        "\n",
        "\n",
        "# Now for the actual Hebbian update verification:\n",
        "\n",
        "# We look at the PyTorch-computed gradient for the weight at kernel position 1,1, input channel 0, output channel numfilt (looping from 0 to 9).\n",
        "# It should be equal to the proper Hebbian update for the rule considered, summed over all positions at which the filter is applied and all batch elements.\n",
        "\n",
        "z=0\n",
        "\n",
        "print(\"The two columns in the following should be roughly identical:\")\n",
        "for numfilt in range(10):\n",
        "    z = 0\n",
        "    for bpos in range(BATCHSIZE):\n",
        "        for col in range(xshape[2]):\n",
        "            for row in range(xshape[3]):\n",
        "                if col % STRIDES[0] == 0 and row % STRIDES[0] == 0:\n",
        "                    #print(\"adding:\")\n",
        "                    if RULE == \"PLAINHEBB\":\n",
        "                        z += ry[bpos, numfilt, col // STRIDES[0], row // STRIDES[0]] * xx[bpos, 0, col+1, row+1] \n",
        "                    elif RULE == \"INSTAR\":\n",
        "                        z += ry[bpos, numfilt, col // STRIDES[0], row // STRIDES[0]] * (xx[bpos, 0, col+1, row+1] - w0[numfilt,0,1,1])\n",
        "                    elif RULE == \"OJA\":\n",
        "                        z += ry[bpos, numfilt, col // STRIDES[0], row // STRIDES[0]] * (xx[bpos, 0, col+1, row+1] - w0[numfilt,0,1,1] * ry[bpos, numfilt, col // STRIDES[0], row // STRIDES[0]])\n",
        "                    else:\n",
        "                        raise ValueError(\"Which Rule?\")\n",
        "\n",
        "\n",
        "    print(g0[numfilt, 0, 1, 1], -z) #, w0[numfilt, 0, 0, 0], xx[0,0,0,0])\n",
        "#for posmaxy in range(10):\n",
        "#    z = -xx[0, 0, 0, 0] * py[0, posmaxy, 0, 0]\n",
        "#    print(g0[posmaxy, 0, 0, 0], z, w0[posmaxy, 0, 0, 0], xx[0,0,0,0])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w.shape: (100, 3, 5, 5) grad.shape: (100, 3, 5, 5) x.shape: (27, 3, 32, 32) py.shape: (27, 100, 3, 3)\n",
            "-49.826347\n",
            "Actual py (channel 0, pos 0,0, batch position 0): 1.0887709\n",
            "Computed py: 0.5310140330693685\n",
            "Position max prelimy (batch pos 0, pos 0,0): 72  - py at this pos: 1.7010709\n",
            "======\n",
            "The two columns in the following should be roughly identical:\n",
            "-47.784554 -47.78455396741629\n",
            "-1.3501656 -1.3501656651496887\n",
            "-1.2514424 -1.2514425963163376\n",
            "-4.815675 -4.815674617886543\n",
            "-17.923779 -17.923781782388687\n",
            "9.121071 9.121071457862854\n",
            "3.9969635 3.996963679790497\n",
            "0.70757526 0.707575336098671\n",
            "15.537903 15.537902865558863\n",
            "0.273444 0.2734439969062805\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}